{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "questions_answers = []\n",
    "language_dataset = '../trivia_qa_polish.csv'\n",
    "\n",
    "# Open the CSV file with the appropriate encoding\n",
    "with open(language_dataset, mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        question, pre_configured_answer = row\n",
    "        questions_answers.append((question, pre_configured_answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel, pipeline\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Remove warning messages\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "class Evaluator: # example of conciseness, completeness, faithfulness\n",
    "    def __init__(self):\n",
    "        # Load models that will be used across evaluation functions to avoid reloading multiple times\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.nli_model = pipeline(\"text-classification\", model=\"roberta-large-mnli\")\n",
    "\n",
    "    def calculate_completeness(self, reference, generated):\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        score = scorer.score(reference, generated)\n",
    "        completeness_score = score['rougeL'].recall\n",
    "        return completeness_score\n",
    "\n",
    "    def calculate_faithfulness(self, reference, generated):\n",
    "        result = self.nli_model(f\"{reference} [SEP] {generated}\")\n",
    "        label = result[0]['label']\n",
    "        \n",
    "        if label == 'ENTAILMENT':\n",
    "            return 1.0\n",
    "        elif label == 'CONTRADICTION':\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 0.5\n",
    "\n",
    "    def calculate_conciseness(self, reference, generated):\n",
    "        # Tokenize and get embeddings\n",
    "        ref_tokens = self.tokenizer(reference, return_tensors='pt')\n",
    "        gen_tokens = self.tokenizer(generated, return_tensors='pt')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ref_emb = self.model(**ref_tokens).last_hidden_state.mean(dim=1)\n",
    "            gen_emb = self.model(**gen_tokens).last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        similarity = cosine_similarity(ref_emb, gen_emb)\n",
    "        length_penalty = 0.01 * abs(len(generated) - len(reference))\n",
    "        \n",
    "        return similarity - length_penalty\n",
    "\n",
    "    def evaluate_response(self, reference, generated):\n",
    "        conciseness_score = self.calculate_conciseness(reference, generated)\n",
    "        completeness_score = self.calculate_completeness(reference, generated)\n",
    "        faithfulness_score = self.calculate_faithfulness(reference, generated)\n",
    "        \n",
    "        combined_score = {\n",
    "            \"Conciseness\": conciseness_score,\n",
    "            \"Completeness\": completeness_score,\n",
    "            \"Faithfulness\": faithfulness_score\n",
    "        }\n",
    "        return combined_score\n",
    "\n",
    "class LLM_Client:\n",
    "    def __init__(self, api_key, base_url):\n",
    "        self.LLM_Client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "\n",
    "    def generate_response(self, question):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Force each answer to be less than 100 tokens no matter what. Do not accept answers longer than 100 tokens. You are a helpful assistant helping answer questions as briefly, concisely and accurately as possible.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        response = self.LLM_Client.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\",\n",
    "            messages=messages,\n",
    "            max_tokens=100\n",
    "        )\n",
    "\n",
    "        llm_answer = response.choices[0].message.content.strip()\n",
    "        return llm_answer\n",
    "\n",
    "class ResponseManager:\n",
    "    def __init__(self):\n",
    "        self.all_results = []\n",
    "\n",
    "    def format_scores(self, scores):\n",
    "        prettified_scores = {}\n",
    "        for k, v in scores.items():\n",
    "            if isinstance(v, (float, int)):\n",
    "                prettified_scores[k] = round(v, 3)\n",
    "            elif hasattr(v, 'tolist'):\n",
    "                list_value = v.tolist()\n",
    "                prettified_scores[k] = round(list_value[0][0], 3) if isinstance(list_value, list) and isinstance(list_value[0], list) else round(list_value, 3)\n",
    "            else:\n",
    "                prettified_scores[k] = v\n",
    "        return prettified_scores\n",
    "\n",
    "    def add_result(self, question, pre_configured_answer, llm_answer, scores):\n",
    "        prettified_scores = self.format_scores(scores)\n",
    "        result_entry = {\n",
    "            \"Question\": question,\n",
    "            \"Pre-configured Answer\": pre_configured_answer,\n",
    "            \"LLM Answer\": llm_answer,\n",
    "            \"Evaluation Scores\": prettified_scores\n",
    "        }\n",
    "        self.all_results.append(result_entry)\n",
    "\n",
    "    def save_results(self, output_filename=\"evaluation_results.json\"):\n",
    "        with open(output_filename, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(self.all_results, outfile, indent=4, ensure_ascii=False)\n",
    "        print(f\"All results have been saved to {output_filename}\")\n",
    "\n",
    "    def visualise_results(self, scores, question, pre_configured_answer, llm_answer):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Evaluation Scores:\")\n",
    "        print(self.format_scores(scores))\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Question:\\n{question}\\n\")\n",
    "        print(f\"Pre-configured Answer:\\n{pre_configured_answer}\\n\")\n",
    "        print(f\"LLM Answer:\\n{llm_answer}\\n\")\n",
    "        print(\"=\" * 60, \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    load_dotenv()\n",
    "    MY_ENV_VAR = os.getenv('KEY') # follow https://stackoverflow.com/questions/40216311/reading-in-environment-variables-from-an-environment-file for key setup\n",
    "    api_key = MY_ENV_VAR\n",
    "    base_url = \"https://api.aimlapi.com\"\n",
    "\n",
    "\n",
    "    LLM_Client = LLM_Client(api_key=api_key, base_url=base_url)\n",
    "    evaluator = Evaluator()\n",
    "    response_manager = ResponseManager()\n",
    "\n",
    "    # Loop through the questions and evaluate the responses\n",
    "    for question, pre_configured_answer in questions_answers:\n",
    "        llm_answer = LLM_Client.generate_response(question)\n",
    "        scores = evaluator.evaluate_response(pre_configured_answer, llm_answer)\n",
    "        response_manager.add_result(question, pre_configured_answer, llm_answer, scores)\n",
    "        visualised_results = response_manager.visualise_results(scores, question, pre_configured_answer, llm_answer)\n",
    "    # Save the results to a JSON file\n",
    "    response_manager.save_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
